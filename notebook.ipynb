{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to split train data into labels and features\n",
    "train_labels = train_data['label'].to_numpy()\n",
    "train_features = train_data['text'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_data['label'].to_numpy()\n",
    "test_features = test_data['text'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords from train_features\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in range(len(train_features)):\n",
    "    word_tokens = train_features[i].split()\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    train_features[i] = ' '.join(filtered_sentence)\n",
    "\n",
    "for i in range(len(test_features)):\n",
    "    word_tokens = test_features[i].split()\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    test_features[i] = ' '.join(filtered_sentence)\n",
    "\n",
    "print(train_features)\n",
    "print(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "for i in range(len(train_features)):\n",
    "    train_features[i] = train_features[i].lower()[:100]\n",
    "for i in range(len(test_features)):\n",
    "    test_features[i] = test_features[i].lower()[:100]\n",
    "tokenizer.fit_on_texts(train_features)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_features)\n",
    "train_padded = pad_sequences(train_sequences, padding='post').astype(np.float32)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_features)\n",
    "test_padded = pad_sequences(test_sequences, padding='post').astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print distinct labels\n",
    "print(np.unique(train_labels))\n",
    "print(np.unique(test_labels))\n",
    "\n",
    "# convert labels to distinct integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_labels)\n",
    "train_labels = label_encoder.transform(train_labels)\n",
    "test_labels = label_encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_padded.shape)\n",
    "#pad train_padded to have same shape as test_padded\n",
    "test_padded = np.pad(test_padded, ((0,0), (0, 25 - test_padded.shape[1])), 'constant')\n",
    "print(test_padded.shape)\n",
    "print(train_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data\n",
    "train_padded = train_padded / 10000\n",
    "test_padded = test_padded / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_padded)\n",
    "print(train_labels)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From train data select only those entries that have label 1 or 5\n",
    "train_padded = train_padded[np.where((train_labels == 1) | (train_labels == 5))]\n",
    "train_labels = train_labels[np.where((train_labels == 1) | (train_labels == 5))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=10000, output_dim=64, input_length=25),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')  # Or adjust for multiclass\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, 25))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_padded, train_labels, epochs=30, validation_data=(test_padded, test_labels), verbose=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
